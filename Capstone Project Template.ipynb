{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Immigration Study Case\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project is aimed to create databases for immigration in USA and local tempeatures in USA. Starting from gathering data from multiple sources then applying some data cleaning and wrangling steps to make data more suitable for our purposes. At The end all data are stored in parquet files partitioned by suitable columns, which can be later easily retrieved and further processed or integrated. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext, GroupedData\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf, col, to_timestamp, monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, from_unixtime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType, IntegerType\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this project we will study the immigration to USA and answer two question at the end:\n",
    "1. The top 5 nationalities immigrated to USA\n",
    "2. The best 5 cities to immigrants in USA\n",
    "\n",
    "Tools: Apache Spark \n",
    "\n",
    "#### Describe and Gather Data \n",
    "* I94 Immigration Data: This data comes from the US National Tourism and Trade Office  (https://travel.trade.gov/research/reports/i94/historical/2016.html).\n",
    "* World Temperature Data: https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read immig. data in spark\n",
    "fpath1 = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_imm =spark.read.format('com.github.saurfang.sas.spark').load(fpath1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read temperature data in spark\n",
    "fpath2 = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temp =spark.read.format('csv').option(\"header\", \"true\").load(fpath2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_imm.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imm.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8599212"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|       admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null|1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null| 3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS| 6.66643185E8|   93|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_imm.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open SAS file to extract data from it\n",
    "with open('./I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    f_content = f.read()\n",
    "    f_content = f_content.replace('\\t', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_mapper(file, idx):\n",
    "    \"\"\"\n",
    "    create dictionaries for all abbreviations in SAS _description file\n",
    "    Arguments: \n",
    "        file: opened file\n",
    "        idx: index name\n",
    "    \"\"\"\n",
    "    f_content2 = f_content[f_content.index(idx):]\n",
    "\n",
    "    f_content2 = f_content2[:f_content2.index(';')].split('\\n')\n",
    "\n",
    "    f_content2 = [i.replace(\"'\", \"\") for i in f_content2]\n",
    "\n",
    "    dic = [i.split('=') for i in f_content2[1:]]\n",
    "\n",
    "    dic = dict([i[0].strip(), i[1].strip()] for i in dic if len(i) == 2)\n",
    "\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries\n",
    "i94cit_res = code_mapper(f_content, \"i94cntyl\")\n",
    "\n",
    "i94port = code_mapper(f_content, \"i94prtl\")\n",
    "\n",
    "i94mode = code_mapper(f_content, \"i94model\")\n",
    "\n",
    "i94addr = code_mapper(f_content, \"i94addrl\")\n",
    "\n",
    "i94visa = {'1':'Business',\n",
    "\n",
    "    '2': 'Pleasure',\n",
    "\n",
    "    '3' : 'Student'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UDFs to look up the entities from the df in the dictionaries set\n",
    "\n",
    "origin_code_udf= udf(lambda x: i94cit_res[str(x)],StringType())\n",
    "port_name_code_udf= udf(lambda x: i94port[str(x)],StringType())\n",
    "destin_code_udf= udf(lambda x: i94addr[str(x)],StringType())\n",
    "transport_code_udf= udf(lambda x: i94mode[str(x)],StringType())\n",
    "visa_udf= udf(lambda x: i94visa[str(x)],StringType())\n",
    "upper_udf= udf(lambda x: x.upper(),StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return key for any value \n",
    "@udf()\n",
    "def get_abbrev(val): \n",
    "    for key, value in i94addr.items(): \n",
    "        if str(val).upper() == value: \n",
    "            return key \n",
    "\n",
    "    return \"key doesn't exist\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "we will filter data and remove duplicates for each dataset. In addidion missing data will be dropped\n",
    "#### Cleaning Steps\n",
    "* Dropping missing data and duplicates \n",
    "* Filteration of data based on the country and othe aspects of this study\n",
    "* Casting new created or already defined columns with the right data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping null, empty and duplicates in the immigration dataframe\n",
    "# Also casting columns with right data type\n",
    "df_imm_valid= df_imm.dropna(how=\"any\", subset= [\"i94addr\", \"i94port\",\"cicid\",\"i94res\"]).dropDuplicates([\"cicid\"])\\\n",
    ".withColumn(\"i94res\",df_imm[\"i94res\"].cast(IntegerType()))\\\n",
    ".withColumn(\"i94visa\",df_imm[\"i94visa\"].cast(IntegerType()))\\\n",
    ".withColumn(\"i94mode\",df_imm[\"i94mode\"].cast(IntegerType()))\\\n",
    ".withColumn(\"i94yr\",df_imm[\"i94yr\"].cast(IntegerType()))\\\n",
    ".withColumn(\"cicid\",df_imm[\"cicid\"].cast(IntegerType()))\\\n",
    ".withColumn(\"biryear\",df_imm[\"biryear\"].cast(IntegerType()))\\\n",
    ".withColumn(\"i94bir\",df_imm[\"i94bir\"].cast(IntegerType()))\\\n",
    ".withColumn(\"i94mon\",df_imm[\"i94mon\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filteration of data based on the SAS data and extracting the date from arrdate and depdate columns\n",
    "# Also, applying the defined UDFs to the dataframe\n",
    "i94_immig= df_imm_valid.filter(col(\"i94res\").isin(list(i94cit_res.keys())))\\\n",
    ".filter(col(\"i94addr\").isin(list(i94addr.keys())))\\\n",
    ".filter(col(\"i94port\").isin(list(i94port.keys())))\\\n",
    ".filter(col(\"i94mode\").isin(list(i94mode.keys())))\\\n",
    ".filter(col(\"i94visa\").isin(list(i94visa.keys())))\\\n",
    ".withColumn(\"origin_country\",origin_code_udf(df_imm_valid[\"i94res\"]))\\\n",
    ".withColumn(\"destination\",destin_code_udf(df_imm_valid[\"i94addr\"]))\\\n",
    ".withColumn(\"city_port_name\",port_name_code_udf(df_imm_valid[\"i94port\"]))\\\n",
    ".withColumn(\"transportation\",transport_code_udf(df_imm_valid[\"i94mode\"]))\\\n",
    ".withColumn(\"visa\",visa_udf(df_imm_valid[\"i94visa\"]))\\\n",
    ".withColumn(\"arrdate\", f.date_format(df_imm_valid[\"arrdate\"].cast(dataType=t.TimestampType()), \"yyyy-MM-dd\"))\\\n",
    ".withColumn(\"depdate\", f.date_format(df_imm_valid[\"depdate\"].cast(dataType=t.TimestampType()), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataset to view immigration data by state and origin country\n",
    "I94_Data= i94_immig.select(\"cicid\",col(\"i94yr\").alias(\"year\"),col(\"i94mon\").alias(\"month\"), \"origin_country\",\\\n",
    "                          \"i94port\", \"city_port_name\", \"destination\", \"transportation\",\"depdate\", \"arrdate\",\\\n",
    "                           col(\"count\").alias(\"number_of_entry\"), \"visa\", col(\"i94bir\").alias(\"age\"),\\\n",
    "                          \"gender\",col(\"biryear\").alias(\"year_of_birth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cicid: int, year: int, month: int, origin_country: string, i94port: string, city_port_name: string, destination: string, transportation: string, depdate: string, arrdate: string, number_of_entry: double, visa: string, age: int, gender: string, year_of_birth: int]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to speed up spark with defined df\n",
    "I94_Data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+--------------+-------+-----------------+-----------+--------------+----------+----------+---------------+--------+---+------+-------------+\n",
      "|cicid|year|month|origin_country|i94port|   city_port_name|destination|transportation|   depdate|   arrdate|number_of_entry|    visa|age|gender|year_of_birth|\n",
      "+-----+----+-----+--------------+-------+-----------------+-----------+--------------+----------+----------+---------------+--------+---+------+-------------+\n",
      "|  299|2016|    4|       AUSTRIA|    NYC|     NEW YORK, NY|   NEW YORK|           Air|1970-01-01|1970-01-01|            1.0|Pleasure| 54|  null|         1962|\n",
      "|  305|2016|    4|       AUSTRIA|    NYC|     NEW YORK, NY|   NEW YORK|           Air|1970-01-01|1970-01-01|            1.0|Pleasure| 63|  null|         1953|\n",
      "|  496|2016|    4|       AUSTRIA|    CHI|      CHICAGO, IL|   ILLINOIS|           Air|1970-01-01|1970-01-01|            1.0|Business| 64|  null|         1952|\n",
      "|  558|2016|    4|       AUSTRIA|    SFR|SAN FRANCISCO, CA| CALIFORNIA|           Air|1970-01-01|1970-01-01|            1.0|Business| 42|     M|         1974|\n",
      "|  596|2016|    4|       AUSTRIA|    NAS|  NASSAU, BAHAMAS|    FLORIDA|           Air|1970-01-01|1970-01-01|            1.0|Pleasure| 24|     M|         1992|\n",
      "+-----+----+-----+--------------+-------+-----------------+-----------+--------------+----------+----------+---------------+--------+---+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "I94_Data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping null, empty and duplicates in the immigration dataframe\n",
    "df_temp_valid= df_temp.dropna(how=\"any\", subset= [\"Country\", \"City\",\"AverageTemperature\"]).dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filteration of data based on United States data and appling udf function\n",
    "df_temp= df_temp.filter(df_temp[\"Country\"]==\"United States\")\\\n",
    ".filter(df_temp[\"AverageTemperature\"]!=\"null\")\\\n",
    ".withColumn(\"City\",upper_udf(df_temp[\"City\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Temperatures=df_temp.filter(df_temp[\"City\"].isin(list(i94addr.values())))\\\n",
    ".filter(col(\"City\").isNotNull())\\\n",
    ".withColumn(\"year\",year(df_temp[\"dt\"]))\\\n",
    ".withColumn(\"month\",month(df_temp[\"dt\"]))\\\n",
    ".withColumn(\"avg_temp_fahrenheit\",df_temp[\"AverageTemperature\"]*9/5+32)\\\n",
    ".withColumn(\"i94port\",get_abbrev(df_temp[\"City\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Temperatures=Temperatures.select(\"year\",\"month\",round(col(\"AverageTemperature\"),1).alias(\"avg_temp_celcius\"),\\\n",
    "                                      round(col(\"avg_temp_fahrenheit\"),1).alias(\"avg_temp_fahrenheit\"),\n",
    "                                       \"i94port\",\"City\",\"Country\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, month: int, avg_temp_celcius: double, avg_temp_fahrenheit: double, i94port: string, City: string, Country: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Temperatures.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### Fact table: immigration with columns\n",
    "* cicid \n",
    "* i94yr \n",
    "* i94mon\n",
    "* i94res (origin_country)\n",
    "* i94port\n",
    "* arrdate \n",
    "* i94addr (destination)\n",
    "* depdate\n",
    "* i94mode (transportation)\n",
    "\n",
    "#### Dimension tables: \n",
    "1. immigrant_table\n",
    "* cicid\n",
    "* i94bir\n",
    "* count\n",
    "* i94visa\n",
    "* biryear\n",
    "* gender\n",
    "\n",
    "2. temperature\n",
    "* year\n",
    "* month\n",
    "* avg_temp_celcius\n",
    "* avg_temp_fahrenheit\n",
    "* state_abbrev (i94port)\n",
    "* City\n",
    "* Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I94_Data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write immigration dimension table to parquet files partitioned by i94port\n",
    "I94_Data.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"output/immigration.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "#df_spark.write.parquet(\"sas_data\")\n",
    "df_spark=spark.read.parquet(\"sas_data/part-00000-b9542815-7a8d-45fc-9c67-c9c5007ad0d4-c000.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Temperatures.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Temperatures.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"output/temperature.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "We need a data model which enbales flexible queries to be run. I chose the relational model (SQL) to build a star schema to store our data. It is very common to use SQL since we might change our queries in the future. \n",
    "##### At the beginning we would like to know the most visited cities in USA and the top nationalities immigrating to USA.\n",
    "* Query1: The top 5 nationalities immigrated to USA.\n",
    "* Query2: The best 5 cities to immigrants in USA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tables based on the previous cleaned dataframes  \n",
    "I94_Data.createOrReplaceTempView(\"immigration\")\n",
    "Temperatures.createOrReplaceTempView(\"temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the fact table by joining the immigration and temperature views\n",
    "immigration_table = spark.sql('''\n",
    "SELECT immigration.cicid as cicid,\n",
    "       immigration.year as year,\n",
    "       immigration.month as month,\n",
    "       immigration.origin_country as origin_country,\n",
    "       immigration.i94port as i94port,\n",
    "       immigration.destination as destination,\n",
    "       immigration.arrdate as arrival_date,\n",
    "       immigration.depdate as departure_date,\n",
    "       immigration.transportation as transportation\n",
    "FROM immigration\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write immigration_table to parquet files partitioned by i94port\n",
    "immigration_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"output/immigration_table.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dimension table immigrant\n",
    "immigrant_table = spark.sql('''\n",
    "SELECT immigration.cicid as cicid,\n",
    "       immigration.age as age,\n",
    "       immigration.number_of_entry as number_of_entry,\n",
    "       immigration.visa as reason,\n",
    "       immigration.year_of_birth as year_of_birth,\n",
    "       immigration.gender as gender\n",
    "FROM immigration\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write immigrant_table to parquet files partitioned by reason\n",
    "immigrant_table.write.mode(\"append\").partitionBy(\"reason\").parquet(\"output/immigrant_table.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dimension table temperature\n",
    "temperature_table= spark.sql('''\n",
    "SELECT temperature.year as year,\n",
    "       temperature.month as month,\n",
    "       temperature.avg_temp_celcius as avg_temp_celcius,\n",
    "       temperature.avg_temp_fahrenheit as avg_temp_fahrenheit,\n",
    "       temperature.i94port as i94port,\n",
    "       temperature.City as city,\n",
    "       temperature.Country as country\n",
    "FROM temperature\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write temperature_table to parquet files partitioned by year\n",
    "temperature_table.write.mode(\"append\").partitionBy(\"year\").parquet(\"output/temperature_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|total_no_immig|      origin_country|\n",
      "+--------------+--------------------+\n",
      "|        350055|      UNITED KINGDOM|\n",
      "|        234093|               JAPAN|\n",
      "|        180334|              FRANCE|\n",
      "|        165762|MEXICO Air Sea, a...|\n",
      "|        163555|          CHINA, PRC|\n",
      "+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query1: The top 5 nationalities immigrated to USA\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*) as total_no_immig, origin_country\n",
    "FROM immigration  \n",
    "Group by origin_country\n",
    "ORDER BY total_no_immig DESC \"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|top_cities|destination|\n",
      "+----------+-----------+\n",
      "|    621701|    FLORIDA|\n",
      "|    553677|   NEW YORK|\n",
      "|    470386| CALIFORNIA|\n",
      "|    168764|     HAWAII|\n",
      "|    134321|      TEXAS|\n",
      "+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query2: The best 5 cities to immigrants in USA\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*) as top_cities, destination\n",
    "FROM immigration  \n",
    "Group by destination\n",
    "ORDER BY top_cities DESC \"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "def quality_check(df, table):\n",
    "    '''\n",
    "    Input: Spark dataframe, description of Spark datafram\n",
    "    Output: Print outcome of data quality check\n",
    "    '''\n",
    "    \n",
    "    result = df.count()\n",
    "    if result == 0:\n",
    "        print(\"Data quality check failed for {} with zero records\".format(table))\n",
    "    else:\n",
    "        print(\"Data quality check passed for {} with {} records\".format(table, result))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check passed for temperature with 6238 records\n"
     ]
    }
   ],
   "source": [
    "quality_check(Temperatures, \"temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check passed for immigration with 2917199 records\n"
     ]
    }
   ],
   "source": [
    "quality_check(I94_Data, \"immigration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "#### Fact table: immigration with columns\n",
    "* cicid >> a unique number for the immigrants.\n",
    "* i94yr >> 4 digit year\n",
    "* i94mon >> Numeric month\n",
    "* i94res >> is country from where one has travelled.\n",
    "* i94port >> 3 character code of destination USA city.\n",
    "* arrdate >> is date of arrrival.\n",
    "* i94addr >> is where the immigrants resides in USA.\n",
    "* depdate >> the Departure Date from the USA\n",
    "* i94mode >> 1 digit travel code (transportation)\n",
    "\n",
    "#### Dimension tables: \n",
    "1. immigrant_table\n",
    "* cicid >> a unique number for the immigrants.\n",
    "* i94bir >> Age of respondent in years.\n",
    "* count >> no. of entry.\n",
    "* i94visa >> is the type of visa which one owns.\n",
    "* biryear >> 4 digit year of birth\n",
    "* gender >> gender\n",
    "\n",
    "2. temperature\n",
    "* year >> 4 digit year\n",
    "* month >> Numeric month\n",
    "* avg_temp_celcius >> temperature in °C\n",
    "* avg_temp_fahrenheit >>  temperature in °F\n",
    "* i94port >> 3 character code of destination USA city\n",
    "* City >> city name\n",
    "* Country >> country name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "###### In this project I have mainly used:\n",
    "1. Apache Spark: a very powerful data-handling tool, which enables processing big data from various data sources (e.g. SAS, CSV, JSON, XML ...etc.) in a convenient and easy way. In addition it enables distribution of data on mulitple CPUs and well-suited for expanding data on AWS with EMR clusters.\n",
    "2. Jupyter Notebook: It has been applied in this project for buliding data pipelines because it has the power to display the data in a quick and beautiful way with less effort. It can also be used to integrate many libraries to serve our target (e.g. SQL, Spark, Pandas, Numpy .... etc.) \n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "Data should be monthly updated since the data is defined mothly in the big dataframe\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "    * It would be a good solution to still using spark but move it on AWS by creating EMR cluster.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    * We could use Airflow for this purpose with defined DAGs to be updated daily.\n",
    " * The database needed to be accessed by 100+ people.\n",
    "    * Move the project to AWS and create user credentials for each user and defininig appropriate roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
